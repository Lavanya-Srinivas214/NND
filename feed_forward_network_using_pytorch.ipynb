{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6dEsPFJBE1j",
        "outputId": "df5cb008-5173-4d27-d178-5cb3daa06879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output z: [0.13000001 0.34000003]\n",
            "Activation Output a: [0.5324543  0.58419055]\n",
            "Final output: 0.5821846723556519\n",
            "Loss: 0.17456965148448944\n",
            "Updated w2: [0.21082287 0.31187448]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.tensor([0.5, 0.2, 0.1], dtype=torch.float32)\n",
        "w = torch.tensor([[0.1, 0.4],\n",
        "                  [0.2, 0.3],\n",
        "                  [0.3, 0.6]], dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor([0.01, 0.02], dtype=torch.float32, requires_grad=True)\n",
        "z = torch.matmul(x, w) + b\n",
        "a = torch.sigmoid(z)\n",
        "print(\"Output z:\", z.detach().numpy())\n",
        "print(\"Activation Output a:\", a.detach().numpy())\n",
        "w2 = torch.tensor([0.2, 0.3], dtype=torch.float32, requires_grad=True)\n",
        "b2 = torch.tensor([0.05], dtype=torch.float32, requires_grad=True)\n",
        "z2 = torch.dot(a, w2) + b2\n",
        "a2 = torch.sigmoid(z2)\n",
        "print(\"Final output:\", a2.item())\n",
        "y = torch.tensor(1.0)\n",
        "loss = (y - a2) ** 2\n",
        "print(\"Loss:\", loss.item())\n",
        "loss.backward()\n",
        "lr = 0.1\n",
        "with torch.no_grad():\n",
        "    w2 -= lr * w2.grad\n",
        "print(\"Updated w2:\", w2.detach().numpy())\n"
      ]
    }
  ]
}